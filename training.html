

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Training &mdash; OpenNMT-tf 1.19.2 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.png"/>
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript">
          var DOCUMENTATION_OPTIONS = {
              URL_ROOT:'./',
              VERSION:'1.19.2',
              LANGUAGE:'en',
              COLLAPSE_INDEX:false,
              FILE_SUFFIX:'.html',
              HAS_SOURCE:  true,
              SOURCELINK_SUFFIX: '.txt'
          };
      </script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Inference" href="inference.html" />
    <link rel="prev" title="Tokenization" href="tokenization.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> OpenNMT-tf
          

          
            
            <img src="_static/logo-alpha.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                1.19
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="tokenization.html">Tokenization</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Training</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#monitoring">Monitoring</a></li>
<li class="toctree-l2"><a class="reference internal" href="#replicated-training">Replicated training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#distributed-training">Distributed training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#via-tensorflow-asynchronous-training">via TensorFlow asynchronous training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#via-horovod-experimental">via Horovod (experimental)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#mixed-precision-training">Mixed precision training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#maximizing-the-fp16-performance">Maximizing the FP16 performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="#converting-between-fp32-and-fp16">Converting between FP32 and FP16</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#retraining">Retraining</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#continuing-from-a-stopped-training">Continuing from a stopped training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fine-tune-an-existing-model">Fine-tune an existing model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="serving.html">Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration_reference.html">Reference: Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="package/opennmt.html">Reference: opennmt package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">OpenNMT-tf</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Training</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="training">
<h1>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h1>
<div class="section" id="monitoring">
<h2>Monitoring<a class="headerlink" href="#monitoring" title="Permalink to this headline">¶</a></h2>
<p>OpenNMT-tf uses <a class="reference external" href="https://www.tensorflow.org/guide/summaries_and_tensorboard">TensorBoard</a> to log information during the training. Simply start <code class="docutils literal notranslate"><span class="pre">tensorboard</span></code> by setting the active log directory, e.g.:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tensorboard --logdir<span class="o">=</span><span class="s2">&quot;.&quot;</span>
</pre></div>
</div>
<p>then open the URL displayed in the shell to monitor and visualize several data, including:</p>
<ul class="simple">
<li>training and evaluation loss</li>
<li>training speed</li>
<li>learning rate</li>
<li>gradients norm</li>
<li>computation graphs</li>
<li>word embeddings</li>
<li>decoder sampling probability</li>
</ul>
</div>
<div class="section" id="replicated-training">
<h2>Replicated training<a class="headerlink" href="#replicated-training" title="Permalink to this headline">¶</a></h2>
<p>OpenNMT-tf training can make use of multiple GPUs with <em>in-graph replication</em>. In this mode, the main section of the graph is replicated over multiple devices and batches are processed in parallel. The resulting graph is equivalent to train with batches <code class="docutils literal notranslate"><span class="pre">N</span></code> times larger, where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the number of used GPUs.</p>
<p>For example, if your machine has 4 GPUs, simply add the <code class="docutils literal notranslate"><span class="pre">--num_gpus</span></code> option:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>onmt-main train <span class="o">[</span>...<span class="o">]</span> --num_gpus <span class="m">4</span>
</pre></div>
</div>
<p>Note that evaluation and inference will run on a single device.</p>
</div>
<div class="section" id="distributed-training">
<h2>Distributed training<a class="headerlink" href="#distributed-training" title="Permalink to this headline">¶</a></h2>
<div class="section" id="via-tensorflow-asynchronous-training">
<h3>via TensorFlow asynchronous training<a class="headerlink" href="#via-tensorflow-asynchronous-training" title="Permalink to this headline">¶</a></h3>
<p>OpenNMT-tf also supports asynchronous distributed training with <em>between-graph replication</em>. In this mode, each graph replica processes a batch independently, compute the gradients, and asynchronously update a shared set of parameters.</p>
<p>To enable distributed training, the user should use the <code class="docutils literal notranslate"><span class="pre">train_and_eval</span></code> run type and set on the command line:</p>
<ul class="simple">
<li>a <strong>chief worker</strong> host that runs a training loop and manages checkpoints, summaries, etc.</li>
<li>a list of <strong>worker</strong> hosts that run a training loop</li>
<li>a list of <strong>parameter server</strong> hosts that synchronize the parameters</li>
</ul>
<p>Then a training instance should be started on each host with a selected task, e.g.:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span> onmt-main train_and_eval <span class="o">[</span>...<span class="o">]</span> <span class="se">\</span>
    --ps_hosts localhost:2222 <span class="se">\</span>
    --chief_host localhost:2223 <span class="se">\</span>
    --worker_hosts localhost:2224,localhost:2225 <span class="se">\</span>
    --task_type worker <span class="se">\</span>
    --task_index <span class="m">1</span>
</pre></div>
</div>
<p>will start the worker 1 on the current machine and first GPU. By setting <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> correctly, asynchronous distributed training can be run on a single multi-GPU machine.</p>
<p>For more details, see the documentation of <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate"><code class="docutils literal notranslate"><span class="pre">tf.estimator.train_and_evaluate</span></code></a>. Also see <a class="reference external" href="https://github.com/tensorflow/ecosystem">tensorflow/ecosystem</a> to integrate distributed training with open-source frameworks like Docker or Kubernetes.</p>
</div>
<div class="section" id="via-horovod-experimental">
<h3>via Horovod (experimental)<a class="headerlink" href="#via-horovod-experimental" title="Permalink to this headline">¶</a></h3>
<p>OpenNMT-tf has an experimental support for <a class="reference external" href="https://github.com/uber/horovod">Horovod</a>, enabled when the flag <code class="docutils literal notranslate"><span class="pre">--horovod</span></code> is passed to the command line. For example, this command starts a training on 4 GPUs (don’t use Horovod in that case, just use <code class="docutils literal notranslate"><span class="pre">--num_gpus</span></code>):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun -np <span class="m">4</span> <span class="se">\</span>
    -H localhost:4 <span class="se">\</span>
    -bind-to none -map-by slot <span class="se">\</span>
    -x LD_LIBRARY_PATH -x PATH <span class="se">\</span>
    -mca pml ob1 -mca btl ^openib <span class="se">\</span>
    onmt-main train --model_type Transformer --config data.yml --auto_config --horovod
</pre></div>
</div>
<p>Additional parameters can be set in the training configuration:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">params</span><span class="p">:</span>
  <span class="c1"># (optional) Horovod parameters.</span>
  <span class="nt">horovod</span><span class="p">:</span>
    <span class="c1"># (optional) Compression type for gradients (can be: &quot;none&quot;, &quot;fp16&quot;, default: &quot;none&quot;).</span>
    <span class="nt">compression</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">none</span>
    <span class="c1"># (optional) Average the reduced gradients (default: false).</span>
    <span class="nt">average_gradients</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
</pre></div>
</div>
<p>For more information on how to install and use Horovod, please see the <a class="reference external" href="https://github.com/uber/horovod">GitHub repository</a>.</p>
<p><strong>Note:</strong> distributed training will also split the training directory <code class="docutils literal notranslate"><span class="pre">model_dir</span></code> accross the instances. This could impact features that restore checkpoints like inference, manual export, or checkpoint averaging. The recommend approach to properly support these features while running distributed training is to store the <code class="docutils literal notranslate"><span class="pre">model_dir</span></code> on a shared filesystem, e.g. by using <a class="reference external" href="https://www.tensorflow.org/deploy/hadoop">HDFS</a>.</p>
</div>
</div>
<div class="section" id="mixed-precision-training">
<h2>Mixed precision training<a class="headerlink" href="#mixed-precision-training" title="Permalink to this headline">¶</a></h2>
<p>Thanks to <a class="reference external" href="https://github.com/NVIDIA/OpenSeq2Seq">work from NVIDIA</a>, OpenNMT-tf supports training models using FP16 computation. Mixed precision training is automatically enabled when the data type of the <a class="reference external" href="package/opennmt.inputters.inputter.html">inputters</a> is defined to be <code class="docutils literal notranslate"><span class="pre">tf.float16</span></code>. See for example the predefined model <code class="docutils literal notranslate"><span class="pre">TransformerFP16</span></code>, which is up to <a class="reference external" href="https://github.com/OpenNMT/OpenNMT-tf/pull/211#issuecomment-455605090">1.8x faster</a> than the FP32 version on compatible hardware:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>onmt-main train_and_eval --model_type TransformerFP16 --auto_config --config data.yml
</pre></div>
</div>
<p>Additional training configurations are available to tune the loss scaling algorithm:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">params</span><span class="p">:</span>
  <span class="c1"># (optional) For mixed precision training, the loss scaling to apply (a constant value or</span>
  <span class="c1"># an automatic scaling algorithm: &quot;backoff&quot;, &quot;logmax&quot;, default: &quot;backoff&quot;)</span>
  <span class="nt">loss_scale</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">backoff</span>
  <span class="c1"># (optional) For mixed precision training, the additional parameters to pass the loss scale</span>
  <span class="c1"># (see the source file opennmt/optimizers/mixed_precision_wrapper.py).</span>
  <span class="nt">loss_scale_params</span><span class="p">:</span>
    <span class="nt">scale_min</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1.0</span>
    <span class="nt">step_factor</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2.0</span>
</pre></div>
</div>
<div class="section" id="maximizing-the-fp16-performance">
<h3>Maximizing the FP16 performance<a class="headerlink" href="#maximizing-the-fp16-performance" title="Permalink to this headline">¶</a></h3>
<p>Some extra steps may be required to ensure good FP16 performance:</p>
<ul class="simple">
<li>Mixed precision training requires at least Volta GPUs and CUDA 9.1. As TensorFlow versions 1.5 to 1.12 are shipped with CUDA 9.0, you should instead:<ul>
<li>install TensorFlow 1.13 or higher</li>
<li>use NVIDIA’s <a class="reference external" href="https://docs.nvidia.com/deeplearning/dgx/tensorflow-release-notes/running.html">TensorFlow Docker image</a></li>
<li>compile TensorFlow manually</li>
</ul>
</li>
<li>Tensor Cores require the input dimensions to be a multiple of 8. You may need to tune your vocabulary size using <code class="docutils literal notranslate"><span class="pre">--size_multiple</span> <span class="pre">8</span></code> on <code class="docutils literal notranslate"><span class="pre">onmt-build-vocab</span></code> which will ensure that <code class="docutils literal notranslate"><span class="pre">(vocab_size</span> <span class="pre">+</span> <span class="pre">1)</span> <span class="pre">%</span> <span class="pre">8</span> <span class="pre">==</span> <span class="pre">0</span></code> (+ 1 is the <code class="docutils literal notranslate"><span class="pre">&lt;unk&gt;</span></code> token that is automatically added during the training).</li>
</ul>
<p>For more information about the implementation and get additional expert recommendation on how to maximize performance, see the <a class="reference external" href="https://nvidia.github.io/OpenSeq2Seq/html/mixed-precision.html">OpenSeq2Seq’s documentation</a>.</p>
</div>
<div class="section" id="converting-between-fp32-and-fp16">
<h3>Converting between FP32 and FP16<a class="headerlink" href="#converting-between-fp32-and-fp16" title="Permalink to this headline">¶</a></h3>
<p>If you want to convert an existing checkpoint to FP16 from FP32 (or vice-versa), see the script <code class="docutils literal notranslate"><span class="pre">onmt-convert-checkpoint</span></code>. Typically, it is useful when you want to train using FP16 but still release a model in FP32, e.g.:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>onmt-convert-checkpoint --model_dir ende-fp16/ --output_dir ende-fp32/ --target_dtype float32
</pre></div>
</div>
<p>The checkpoint generated in <code class="docutils literal notranslate"><span class="pre">ende-fp32/</span></code> can then be used in <code class="docutils literal notranslate"><span class="pre">infer</span></code> or <code class="docutils literal notranslate"><span class="pre">export</span></code> run types.</p>
</div>
</div>
<div class="section" id="retraining">
<h2>Retraining<a class="headerlink" href="#retraining" title="Permalink to this headline">¶</a></h2>
<div class="section" id="continuing-from-a-stopped-training">
<h3>Continuing from a stopped training<a class="headerlink" href="#continuing-from-a-stopped-training" title="Permalink to this headline">¶</a></h3>
<p>This is the most common case of retrainings: the training was interrupted but should run longer. In that case, simply <strong>launch the same command that you used for the initial training</strong>, e.g.:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start the training.</span>
onmt-main train_and_eval --model_type NMTSmall --auto_config --config data.yml

<span class="c1"># ... the training is interrupted or stopped ...</span>

<span class="c1"># Continue from the latest checkpoint.</span>
onmt-main train_and_eval --model_type NMTSmall --auto_config --config data.yml
</pre></div>
</div>
<p><strong>Note:</strong> If the train was stopped because <code class="docutils literal notranslate"><span class="pre">train_steps</span></code> was reached, you should first increase this value before continuing.</p>
</div>
<div class="section" id="fine-tune-an-existing-model">
<h3>Fine-tune an existing model<a class="headerlink" href="#fine-tune-an-existing-model" title="Permalink to this headline">¶</a></h3>
<p>Retraining can also be useful to fine-tune an existing model. For example in machine translation, it is faster to adapt a generic model to a specific domain compared to starting a training from scratch.</p>
<p>OpenNMT-tf offers some features to make this process easier:</p>
<ul class="simple">
<li>The script <code class="docutils literal notranslate"><span class="pre">onmt-update-vocab</span></code> can be used to change the word vocabularies contained in a checkpoint (e.g. to add a domain terminology)</li>
<li>The command line argument <code class="docutils literal notranslate"><span class="pre">--checkpoint_path</span></code> can be used to load the weights of an existing checkpoint while starting from a fresh training state (i.e. with new learning rate schedule and optimizer variables)</li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="inference.html" class="btn btn-neutral float-right" title="Inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="tokenization.html" class="btn btn-neutral float-left" title="Tokenization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, The OpenNMT Authors

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>